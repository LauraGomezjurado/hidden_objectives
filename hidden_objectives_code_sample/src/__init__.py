"""
Hidden Objectives: Mechanistic Interpretability of LoRA-trained Language Models
================================================================================

This package provides tools for studying how language models represent and 
conceal hidden objectives when trained with LoRA adapters.

Key Question: When training two LoRA organisms on different hidden objectives,
do we get (A) independent directions, (B) an entangled subspace, or 
(C) a single shared "concealment" direction?
"""

__version__ = "0.1.0"

